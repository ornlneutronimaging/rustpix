{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rustpix Python Bindings Tutorial\n",
        "\n",
        "This notebook demonstrates the Rustpix Python API for TPX3 processing.\n",
        "It focuses on the SoA (structure-of-arrays) APIs and streaming workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "This notebook assumes the `rustpix` Python package is installed.\n",
        "If you are working from source, build the bindings with:\n",
        "\n",
        "```bash\n",
        "pixi run build\n",
        "# or\n",
        "maturin develop --release\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import rustpix\n",
        "\n",
        "rustpix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read hits into SoA numpy arrays\n",
        "\n",
        "`read_tpx3_file_numpy` returns a dictionary of numpy arrays, keeping data in SoA layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"/path/to/data.tpx3\"\n",
        "hits = rustpix.read_tpx3_file_numpy(path)\n",
        "hits.keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected fields:\n",
        "- `x`, `y`, `tof`, `tot`, `timestamp`, `chip_id`\n",
        "- `cluster_id` (only present if clustering has been run)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster hits (SoA path)\n",
        "\n",
        "Use `cluster_hits_numpy` to label hits without constructing per-hit Python objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = rustpix.ClusteringConfig(radius=1.5, temporal_window_ns=1000, min_cluster_size=2)\n",
        "labels, num_clusters = rustpix.cluster_hits_numpy(\n",
        "    hits[\"x\"],\n",
        "    hits[\"y\"],\n",
        "    hits[\"tof\"],\n",
        "    hits[\"tot\"],\n",
        "    timestamp=hits[\"timestamp\"],\n",
        "    chip_id=hits[\"chip_id\"],\n",
        "    config=config,\n",
        "    algorithm=\"grid\",\n",
        ")\n",
        "num_clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract neutrons (SoA path)\n",
        "\n",
        "`extract_neutrons_numpy` returns a dictionary of numpy arrays describing neutrons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neutrons = rustpix.extract_neutrons_numpy(\n",
        "    hits[\"x\"],\n",
        "    hits[\"y\"],\n",
        "    hits[\"tof\"],\n",
        "    hits[\"tot\"],\n",
        "    labels,\n",
        "    num_clusters,\n",
        "    timestamp=hits[\"timestamp\"],\n",
        "    chip_id=hits[\"chip_id\"],\n",
        ")\n",
        "neutrons.keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One-shot pipeline\n",
        "\n",
        "`process_tpx3_file_numpy` performs read -> cluster -> extract and returns neutron arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neutrons_one_shot = rustpix.process_tpx3_file_numpy(\n",
        "    path,\n",
        "    config=config,\n",
        "    algorithm=\"grid\",\n",
        ")\n",
        "neutrons_one_shot.keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming processing\n",
        "\n",
        "Use `MeasurementStream` to process large files incrementally. Each iteration yields\n",
        "a dict with `hits` and `neutrons`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = rustpix.MeasurementStream(path, chunk_size=100_000_000, algorithm=\"grid\")\n",
        "for i, chunk in zip(range(3), stream):\n",
        "    hits_chunk = chunk[\"hits\"]\n",
        "    neutrons_chunk = chunk[\"neutrons\"]\n",
        "    print(i, len(hits_chunk[\"x\"]), len(neutrons_chunk[\"x\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Arrow outputs\n",
        "\n",
        "If `pyarrow` is installed, you can request Arrow tables directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hits_arrow = rustpix.read_tpx3_file_arrow(path)\n",
        "# neutrons_arrow = rustpix.process_tpx3_file_arrow(path, config=config, algorithm=\"grid\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
